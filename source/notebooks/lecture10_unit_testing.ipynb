{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Writing unit tests and running from anywhere on your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related references:\n",
    "\n",
    "- https://docs.python-guide.org/writing/tests/\n",
    "- https://jeffknupp.com/blog/2013/12/09/improve-your-python-understanding-unit-testing/\n",
    "- https://docs.python.org/3/library/unittest.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always want to test that our program works. Last time, we did that manually--I looked at the output and saw it was what I expected and wanted. We also talked about the need to automate tests, so we can be sure that, as we add more functionality, we don't accidentally break something. This will also allow us to check that our program behaves as desired on multiple architectures. Today, we'll discuss how to do so.\n",
    "\n",
    "Writing tests for Python is much like writing tests for your own code. Tests need to be thorough, fast, isolated, consistently repeatable, and as simple as possible. We try to have tests both for normal behavior and for error conditions. Tests live in the `tests` directory, where every file that includes tests has a `test_` prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is meant by unit testing?\n",
    "\n",
    "It is testing small units of code, e.g. individual functions, to help isolate the location of the error.\n",
    "\n",
    "In addition, I always test the \"main\" function to make sure individual units connect correctly.\n",
    "\n",
    "`unittest` is also a test module in the Python standard library. Creating test cases is accomplished by subclassing unittest.TestCase (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of types of errors to look for\n",
    "\n",
    "1. *Syntax errors*: often checks that the user typed in a valid argument, and gives a useful error message if such a mistake is caught.\n",
    "\n",
    "1. *Logical errors*: when the algorithm used is not correct (either originally, or error created upon later editing of the code)\n",
    "\n",
    "1. *Unexpected input/edge cases*: e.g. a program is only meant to work with positive values (e.g. a zero is entered and a log will be taken)\n",
    "\n",
    "What type of error does the test below check for? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A very basic unittest\n",
    "\n",
    "import unittest\n",
    "\n",
    "def fun(x):\n",
    "    return x + 1\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    def test(self):\n",
    "        self.assertEqual(fun(3), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart of the test is the assertion step at the end. There are many options, more even than those listed below. There are further notes on how all the following work at https://docs.python.org/3/library/unittest.html.\n",
    "\n",
    "| Method                     | Checks that                                                                   |\n",
    "|----------------------------|-------------------------------------------------------------------------------|\n",
    "| assertEqual(a, b)          | a == b                                                                        |\n",
    "| assertNotEqual(a, b)       | a != b                                                                        |\n",
    "| assertTrue(x)\t             | bool(x) is True                                                               |\n",
    "| assertFalse(x)             | bool(x) is False                                                              |\n",
    "| assertIs(a, b)             | a is b                                                                        |\n",
    "| assertIsNot(a, b)          | a is not b                                                                    |\n",
    "| assertIsNone(x)            | x is None                                                                     |\n",
    "| assertIsNotNone(x)         | x is not None                                                                 | \n",
    "| assertIn(a, b)             | a in b                                                                        | \n",
    "| assertNotIn(a, b)          | a not in b                                                                    | \n",
    "| assertAlmostEqual(a, b)    | round(a-b, 7) == 0                                                            | \n",
    "| assertNotAlmostEqual(a, b) | round(a-b, 7) != 0                                                            | \n",
    "| assertGreater(a, b)        | a > b                                                                         |\n",
    "| assertGreaterEqual(a, b)   | a >= b                                                                        |\n",
    "| assertLess(a, b)           | a < b                                                                         |\n",
    "| assertLessEqual(a, b)      | a <= b                                                                        |\n",
    "| assertCountEqual(a, b)\t | a and b have the same elements in the same number, regardless of their order  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the functions we are testing will be in a different file than the tests. I've set up tests for our sample arthritis project (https://github.com/team-mayes/arthritis_proj_demo). Let's walk through those tests to get a feel for the types of tests we want to make.\n",
    "\n",
    "First, note that I've reorganized my folders a little bit, separating out `tests` and making a subfolder with files for testing the script `data_proc.py`. I like to have a separate test for each script, and a separate folder for files for testing that script.\n",
    "\n",
    "![Screen shot](images/lect10_reorg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at `test_data_proc.py`. As usual, the beginning includes imports of libraries. Not that we have to import the script that we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unit and regression test for data_proc.py\n",
    "\"\"\"\n",
    "import errno\n",
    "import os\n",
    "import sys\n",
    "import unittest\n",
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import logging\n",
    "from arthritis_proj.data_proc import main, data_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `logging` library is helpful to log or perform certain actions only under certain conditions. You'll see how this is used in the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "DISABLE_REMOVE = logger.isEnabledFor(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refer to where particular files are, we want to use relative paths only, as we want these tests to work on many diffrent machines. What do you think the following lines do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CURRENT_DIR = os.path.dirname(__file__)\n",
    "MAIN_DIR = os.path.join(CURRENT_DIR, '..')\n",
    "TEST_DATA_DIR = os.path.join(CURRENT_DIR, 'data_proc')\n",
    "PROJ_DIR = os.path.join(MAIN_DIR, 'arthritis_proj')\n",
    "DATA_DIR = os.path.join(PROJ_DIR, 'data')\n",
    "SAMPLE_DATA_FILE_LOC = os.path.join(DATA_DIR, 'sample_data.csv')\n",
    "\n",
    "# Assumes running tests from the main directory\n",
    "DEF_CSV_OUT = os.path.join(MAIN_DIR, 'sample_data_stats.csv')\n",
    "DEF_PNG_OUT = os.path.join(MAIN_DIR, 'sample_data_stats.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does it for our preamble. Now I've defined a helper function that will be used by the tests below it. (FYI, as described on https://docs.python.org/3/library/errno.html, `errno.ENOENT` is a standard error message that means `No such file or directory`. Any questions on what this function does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def silent_remove(filename, disable=False):\n",
    "    \"\"\"\n",
    "    Removes the target file name, catching and ignoring errors that indicate that the\n",
    "    file does not exist.\n",
    "\n",
    "    @param filename: The file to remove.\n",
    "    @param disable: boolean to flag if want to disable removal\n",
    "    \"\"\"\n",
    "    if not disable:\n",
    "        try:\n",
    "            os.remove(filename)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.ENOENT:\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to the tests themselves, going through each line to see what they do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestMain(unittest.TestCase):\n",
    "    # These tests make sure that the program can run properly from main\n",
    "    def testSampleData(self):\n",
    "        # Checks that runs with defaults and that files are created\n",
    "        test_input = [\"-c\", DEFAULT_DATA_FILE_LOC]\n",
    "        try:\n",
    "            if logger.isEnabledFor(logging.DEBUG):\n",
    "                main(test_input)\n",
    "            # checks that the expected message is sent to standard out\n",
    "            with capture_stdout(main, test_input) as output:\n",
    "                self.assertTrue(\"sample_data_stats.csv\" in output)\n",
    "\n",
    "            self.assertTrue(os.path.isfile(\"sample_data_stats.csv\"))\n",
    "            self.assertTrue(os.path.isfile(\"sample_data_stats.png\"))\n",
    "        finally:\n",
    "            silent_remove(DEF_CSV_OUT, disable=DISABLE_REMOVE)\n",
    "            silent_remove(DEF_PNG_OUT, disable=DISABLE_REMOVE)\n",
    "            \n",
    "class TestMainFailWell(unittest.TestCase):\n",
    "    def testMissingFile(self):\n",
    "        test_input = [\"-c\", \"ghost.txt\"]\n",
    "        if logger.isEnabledFor(logging.DEBUG):\n",
    "            main(test_input)\n",
    "        with capture_stderr(main, test_input) as output:\n",
    "            self.assertTrue(\"ghost.txt\" in output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`capture_stdout` and `capture_stderr` called functions at the bottom of the file. Don't worry about understanding each line of the bellow. The main point is that these allow any messages sent to standard out or standard error, respectively, to be captured as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "# From http://schinckel.net/2013/04/15/capture-and-test-sys.stdout-sys.stderr-in-unittest.testcase/\n",
    "@contextmanager\n",
    "def capture_stdout(command, *args, **kwargs):\n",
    "    # pycharm doesn't know six very well, so ignore the false warning\n",
    "    # noinspection PyCallingNonCallable\n",
    "    out, sys.stdout = sys.stdout, StringIO()\n",
    "    command(*args, **kwargs)\n",
    "    sys.stdout.seek(0)\n",
    "    yield sys.stdout.read()\n",
    "    sys.stdout = out\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def capture_stderr(command, *args, **kwargs):\n",
    "    # pycharm doesn't know six very well, so ignore the false warning\n",
    "    # noinspection PyCallingNonCallable\n",
    "    err, sys.stderr = sys.stderr, StringIO()\n",
    "    command(*args, **kwargs)\n",
    "    sys.stderr.seek(0)\n",
    "    yield sys.stderr.read()\n",
    "    sys.stderr = err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separately, the tests below checked that the function `data_analysis()` behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestDataAnalysis(unittest.TestCase):\n",
    "    def testSampleData(self):\n",
    "        # Tests that the np array generated by the data_analysis function matches saved expected results\n",
    "        csv_data = np.loadtxt(fname=SAMPLE_DATA_FILE_LOC, delimiter=',')\n",
    "        analysis_results = data_analysis(csv_data)\n",
    "        expected_results = np.loadtxt(fname=os.path.join(TEST_DATA_DIR, \"sample_data_results.csv\"), delimiter=',')\n",
    "        self.assertTrue(np.allclose(expected_results, analysis_results))\n",
    "\n",
    "    def testSampleData2(self):\n",
    "        # A second check, with slightly different values, of the data_analysis function\n",
    "        csv_data = np.loadtxt(fname=os.path.join(TEST_DATA_DIR, \"sample_data2.csv\"), delimiter=',')\n",
    "        analysis_results = data_analysis(csv_data)\n",
    "        expected_results = np.loadtxt(fname=os.path.join(TEST_DATA_DIR, \"sample_data2_results.csv\"), delimiter=',')\n",
    "        self.assertTrue(np.allclose(expected_results, analysis_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another test together. I want to see what happens if I have different number of days of data for different patients. I started with a new file () in which I deleted some days of data from patients 2 and three (want to see what is different? Do a file compare!).\n",
    "\n",
    "![Screen shot](images/lect10_missing-cols.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens when we run this, which I will do from the beginnings of a new test. Note that I moved a test into a new class. I do this to help people who might use my tests to figure out how to run the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestMainBadInput(unittest.TestCase):\n",
    "    def testMissingFile(self):\n",
    "        test_input = [\"-c\", \"ghost.txt\"]\n",
    "        main(test_input)\n",
    "        with capture_stderr(main, test_input) as output:\n",
    "            self.assertTrue(\"ghost.txt\" in output)\n",
    "\n",
    "    def testDataDiffNumCols(self):\n",
    "        input_file = os.path.join(TEST_DATA_DIR, \"sample_data3_diff_cols.csv\")\n",
    "        test_input = [\"-c\", input_file]\n",
    "        main(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `testDataDiffNumCols`, I get the following error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screen shot](images/lect10_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll take this information, specifically the type of error that I'm getting, and do a try-catch loop for this error. I know that this problem happens in line 67 of data_proc.py, so I'll go there (by clicking on the link) to see where I can catch this exception. \n",
    "\n",
    "![Screen shot](images/lect10_error-loc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are already in a try-catch loop, but it only caught an `IOError`. We now can add the so I'll add the `ValueError`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_cmdline(argv):\n",
    "    \"\"\"\n",
    "    Returns the parsed argument list and return code.\n",
    "    `argv` is a list of arguments, or `None` for ``sys.argv[1:]``.\n",
    "    \"\"\"\n",
    "    if argv is None:\n",
    "        argv = sys.argv[1:]\n",
    "\n",
    "    # initialize the parser object:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-c\", \"--csv_data_file\", help=\"The location (directory and file name) of the csv file with \"\n",
    "                                                      \"data to analyze\",\n",
    "                        default=DEFAULT_DATA_FILE_LOC)\n",
    "    args = None\n",
    "    try:\n",
    "        args = parser.parse_args(argv)\n",
    "        args.csv_data = np.loadtxt(fname=args.csv_data_file, delimiter=',')\n",
    "    except IOError as e:\n",
    "        warning(\"Problems reading file:\", e)\n",
    "        parser.print_help()\n",
    "        return args, IO_ERROR\n",
    "    except ValueError as e:\n",
    "        warning(\"Read invalid data:\", e)\n",
    "        parser.print_help()\n",
    "        return args, INVALID_DATA\n",
    "    \n",
    "    return args, SUCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try running our test again.\n",
    "\n",
    "![Screen shot](images/lect10_no-asserts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the behavior we want, but we are not yet actually testing anything with an assert statement. Let's do that by checking that we get the error we expect. *FYI*: I added a clause that the program will be run on its own only if the debugger is on, but we are not getting anything from running it without capturing an error message that we will use for testing. It can be nice to see the whole output message, which we would get if we have make the debugging mode active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestMainFailWell(unittest.TestCase):\n",
    "    def testMissingFile(self):\n",
    "        test_input = [\"-c\", \"ghost.txt\"]\n",
    "        if logger.isEnabledFor(logging.DEBUG):\n",
    "            main(test_input)\n",
    "        with capture_stderr(main, test_input) as output:\n",
    "            self.assertTrue(\"ghost.txt\" in output)\n",
    "\n",
    "    def testDataDiffNumCols(self):\n",
    "        input_file = os.path.join(TEST_DATA_DIR, \"sample_data3_diff_cols.csv\")\n",
    "        test_input = [\"-c\", input_file]\n",
    "        if logger.isEnabledFor(logging.DEBUG):\n",
    "            main(test_input)\n",
    "        with capture_stderr(main, test_input) as output:\n",
    "            self.assertTrue(\"Wrong number of columns\" in output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some best practices in writing tests\n",
    "\n",
    "(from https://docs.python-guide.org/writing/tests/)\n",
    "\n",
    "- A testing unit should focus on one tiny bit of functionality and prove it correct.\n",
    "- Each test unit must be fully independent. Each test must be able to run alone, and also within the test suite, regardless of the order that they are called. The implication of this rule is that each test must be loaded with a fresh dataset and may have to do some cleanup afterwards. \n",
    "- Try hard to make tests that run fast. If one single test needs more than a few milliseconds to run, development will be slowed down or the tests will not be run as often as is desirable. In some cases, tests can’t be fast because they need a complex data structure to work on, and this data structure must be loaded every time the test runs. Keep these heavier tests in a separate test suite that is run by some scheduled task, and run all other tests as often as needed. Use small versions of data and files to be tested to keep things fast.\n",
    "- Learn your tools and learn how to run a single test or a test case. Then, when developing a function inside a module, run this function’s tests frequently, ideally automatically when you save the code.\n",
    "- Always run the full test suite before a coding session, and run it again after. This will give you more confidence that you did not break anything in the rest of the code.\n",
    "- It is a good idea to implement a hook that runs all tests before pushing code to a shared repository.\n",
    "- If you are in the middle of a development session and have to interrupt your work, it is a good idea to write a broken unit test about what you want to develop next. When coming back to work, you will have a pointer to where you were and get back on track faster.\n",
    "- The first step when you are debugging your code is to write a new test pinpointing the bug. While it is not always possible to do, those bug catching tests are among the most valuable pieces of code in your project.\n",
    "- Use long and descriptive names for testing functions. The style guide here is slightly different than that of running code, where short names are often preferred. The reason is testing functions are never called explicitly. square() or even sqr() is ok in running code, but in testing code you would have names such as testSquareOfNumber2(), testSquareNegativeNumber(). These function names are displayed when a test fails, and should be as descriptive as possible.\n",
    "- When something goes wrong or has to be changed, and if your code has a good set of tests, you or other maintainers will rely largely on the testing suite to fix the problem or modify a given behavior. Therefore the testing code will be read as much as or even more than the running code. A unit test whose purpose is unclear is not very helpful in this case.\n",
    "- Another use of the testing code is as an introduction to new developers. When someone will have to work on the code base, running and reading the related testing code is often the best thing that they can do to start. They will or should discover the hot spots, where most difficulties arise, and the corner cases. If they have to add some functionality, the first step should be to add a test to ensure that the new functionality is not already a working path that has not been plugged into the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Start thinking about your project. I encourage discussion with me about what you might want to do and how to accomplish it. I will be posting some more examples to help you with the project; knowing what kinds of tasks you want to accomplish will help me set up useful examples for you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
